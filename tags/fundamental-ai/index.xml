<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fundamental AI on JerryYan&#39;s Blog</title>
    <link>https://jerryyan24.github.io/tags/fundamental-ai/</link>
    <description>Recent content in Fundamental AI on JerryYan&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Jerry Yan</copyright>
    <lastBuildDate>Sat, 25 Mar 2023 15:04:53 +0800</lastBuildDate><atom:link href="https://jerryyan24.github.io/tags/fundamental-ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gradient accumulation principle</title>
      <link>https://jerryyan24.github.io/posts/gradient-accumulation-principle/</link>
      <pubDate>Sat, 25 Mar 2023 15:04:53 +0800</pubDate>
      
      <guid>https://jerryyan24.github.io/posts/gradient-accumulation-principle/</guid>
      <description>Gradient accumulation principle During deep learning training, the batch size of the data is limited by the GPU memory, and the batch size affects the final accuracy of the model and the performance of the training process. As the model gets larger and larger with constant GPU memory, this means that the batch size of the data can only be reduced, and this is where Gradient Accumulation can be a simple solution to the problem.</description>
    </item>
    
  </channel>
</rss>
